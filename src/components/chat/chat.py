import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# åŠ è½½ä½ å¾®è°ƒçš„æ¨¡å‹å’Œ tokenizer
model_name = "your-fine-tuned-model"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# ä¾§è¾¹æ ï¼ˆä¸å†éœ€è¦ API Keyï¼‰
with st.sidebar:
    st.markdown("[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)")


# è®¾ç½®æ ‡é¢˜å’Œç®€ä»‹
st.title("ğŸ‘©â€âš•ï¸ğŸ§‘â€âš•ï¸ åŒ»ç”Ÿä¸æ‚£è€…èŠå¤©")
st.caption("ğŸš€ ä½¿ç”¨å¾®è°ƒæ¨¡å‹çš„ Streamlit åŒ»æ‚£èŠå¤©ç•Œé¢")

# åˆå§‹åŒ–æ¶ˆæ¯åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„åŒ»ç”Ÿï¼Œè¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜éœ€è¦å’¨è¯¢ï¼Ÿ"}]

# æ˜¾ç¤ºèŠå¤©è®°å½•
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # è®°å½•ç”¨æˆ·è¾“å…¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    
    # ä½¿ç”¨å¾®è°ƒæ¨¡å‹ç”Ÿæˆå›å¤
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(inputs.input_ids, max_length=500)
    reply = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # è®°å½•åŒ»ç”Ÿå›å¤
    st.session_state.messages.append({"role": "assistant", "content": reply})
    st.chat_message("assistant").write(reply)
